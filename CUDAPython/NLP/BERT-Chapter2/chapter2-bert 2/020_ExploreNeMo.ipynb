{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 NLP Projects with NVIDIA NeMo\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/nemo/nemo-app-stack.png\" width=350>\n",
    "\n",
    "\n",
    "In this notebook, you'll learn the basics of setting up an NLP project with [NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo).  You'll learn what components are needed, where to find them, and how to instantiate them.  \n",
    "\n",
    "**[2.1 Programming Model](#2.1-Programming-Model)**<br>\n",
    "**[2.2 Create Neural Modules (Step 1)](#2.2-Create-Neural-Modules-(Step-1))**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Data Layer and Tokenizer](#2.2.1-Data-Layer-and-Tokenizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 Language Model and Classifier](#2.2.2-Language-Model-and-Classifier)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Loss Function](#2.2.3-Loss-Function)<br>\n",
    "**[2.3 Create Neural Graph (Step 2)](#2.3-Create-Neural-Graph-(Step-2))**<br>\n",
    "**[2.4 Start an Action (Step 3)](#2.4-Start-an-Action-(Step-3))**<br>\n",
    "\n",
    "NeMo is an open source, high-performance toolkit for building conversational AI applications using a PyTorch backend.  NeMo provides a level of abstraction beyond Keras or PyTorch, that makes it possible to easily compose complex neural network architectures using reusable components called _neural modules_ (hence the name NeMo). NeMo includes capabilities to scale training to multi-GPU systems and multi-node clusters as well, and provides a straightforward path to model deployment for production real-time inference. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Programming Model\n",
    "NeMo includes library collections for all aspects of the conversational AI pipeline: speech recognition (`nemo_asr`), natural language (`nemo_nlp`), and speech synthesis (`nemo_tts`).  For our projects, we'll use the `nemo-nlp` and some core libraries.\n",
    "\n",
    "Building an application using NeMo APIs consists of three basic steps:\n",
    "1. **Create neural modules** after instantiating the NeuralModuleFactory:<br>\n",
    "A _NeuralModule_ is an abstraction between a network layer and a full neural network, for example: encoder, decoder, language model, acoustic model, etc.  \n",
    "1. **Create the neural graph** of tensors connecting the neural modules together:<br>\n",
    "Define all the inputs, outputs, and connections you need to form a pipeline.\n",
    "1. **Start an \"action\"** such as `train` for training a network or `infer` obtain a result from a network:<br>\n",
    "We can also define _callbacks_ for evaluation, logging and performance monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the nemo and nlp imports (Ignore the torchaudio error if it appears).\n",
    "# Import \"inspect\" so we can look at method signatures.\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Create Neural Modules (Step 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating any neural modules, we must instantiate the [NeuralModuleFactory](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/v0.11.0/api-docs/nemo.html?highlight=NeuralModuleFactory#nemo.core.neural_factory.NeuralModuleFactory) object.  There are a number of settings that we'll use in the projects to specify directories for intermediate information such as checkpoints and logs.  This is how to instantiate the factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the NeuralModuleFactory before creating neural modules.\n",
    "nf = nemo.core.NeuralModuleFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Modules can be conceptually classified into four potentially overlapping categories:\n",
    "\n",
    "* Data Layers - modules that perform extract, transform, load, and feed of the data\n",
    "* Trainable Modules - modules that contain trainable weights\n",
    "* Non Trainable Modules - non-trainable module, for example, table lookup, data augmentation, greedy decoder, etc. \n",
    "* Loss Modules - modules that compute loss functions\n",
    "\n",
    "For text classification and NER, we will need a data layer, a trainable language model, a trainable classifier, and a loss module (shown as boxes in the diagram below).  \n",
    "\n",
    "<img src=\"images/nemo/nm-pipe.png\" width=800>\n",
    "     \n",
    "It is possible to build your own neural modules, but there are plenty available for us to use already.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Data Layer and Tokenizer\n",
    "Run the following cell to see a list of available NLP data layer neural modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the callable data layer objects.\n",
    "[method_name for method_name in dir(nemo_nlp.nm.data_layers)\n",
    "                  if callable(getattr(nemo_nlp.nm.data_layers, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification project, we'll use the `BertTextClassificationDataLayer` neural module, while for NER we'll use `BertTokenClassificationDataLayer`.  Take a look at the class signatures below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo_nlp.nm.data_layers.BertTextClassificationDataLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo_nlp.nm.data_layers.BertTokenClassificationDataLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it has a few differences, such as the imported `text_file` and `label_file` parameters needed for NER, rather than the single `input_file` used for the text classification task.\n",
    "\n",
    "In both cases, we need to also provide the `tokenizer` and `max_seq_length` parameters.  For the maximum sequence length, we have a tradeoff here between providing enough information for the system to employ useful attention, while maintaining efficiency of memory and processing time.  The maximum size for BERT is 512. Depending on the particular task and constraints, we might typically choose 32, 64, 128, 256, or 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "A _tokenizer_ segments text and documents into a given type of token, which may be sentences, sentence pieces, words, word pieces, or even characters.  Take a look at the choices currently available within the NeMo library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the callable tokenizer objects.\n",
    "[method_name for method_name in dir(nemo_nlp.data.tokenizers)\n",
    "                  if callable(getattr(nemo_nlp.data.tokenizers, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be starting with pretrained BERT models and fine-tuning from there, our tokenizer must use the one that the BERT model expects, which happens to be the `NemoBertTokenizer`.  In addition to breaking apart the input text, the tokenizer converts each token to an _id_ associated with a vocabulary file.  This vocabulary can either be provided by the user, or must be calculated when the tokenizer is specified, which may take a bit of time.  \n",
    "\n",
    "The details of the tokenizer instantiation can be abstracted somewhat by using the `get_tokenizer` helper.  This is a little easier in practice since it abstracts some details related to exactly which BERT or Megatron pretrained model we choose.  For our example, we'll use `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nemo.collections.nlp.data.tokenizers.get_tokenizer(tokenizer_name='nemobert', \n",
    "                                                               pretrained_model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT tokenizer uses _wordpiece_ tokenization, and since we have specified an uncased model, we would expect the tokenizer to turn all the words to lower case as well.  Run the following cell to see how the tokenizer breaks apart a sentence with a wordpiece algorithm, and encodes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokens from an input\n",
    "test_sentence = 'How about this stupendous sentence?'\n",
    "tokenized = tokenizer.text_to_tokens(test_sentence)\n",
    "encoded = tokenizer.text_to_ids(test_sentence)\n",
    "print('{}\\n{}\\n{}'.format(test_sentence, tokenized, encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate a nominal data layer neural module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl = nemo_nlp.nm.data_layers.BertTextClassificationDataLayer(input_file='data/NCBI_tc-3/dev.tsv', \n",
    "                                                             tokenizer=tokenizer, \n",
    "                                                             max_seq_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Language Model and Classifier\n",
    "Run the following cell to see a list of available NLP trainable neural modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the callable trainable neural module objects\n",
    "[method_name for method_name in dir(nemo_nlp.nm.trainables)\n",
    "                  if callable(getattr(nemo_nlp.nm.trainables, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "We'll use the `get_pretrained_lm_model` method to specify a language model.  We can see a list of available models for us to try.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nemo_nlp.nm.trainables.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to instantiate this type of neural module.  Recall that we already chose 'bert-base-uncased' for the tokenizer. The language model chosen here should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the language model.\n",
    "lm = nemo_nlp.nm.trainables.get_pretrained_lm_model('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "For text classification we'll need the `SequenceClassifier` trainable neural module, while the NER project will require the similar `TokenClassifier` neural module.  Take a look at the signatures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo_nlp.nm.trainables.SequenceClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo_nlp.nm.trainables.TokenClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we must provide the `hidden_size`, which we can pull from the language model, and the number of label categories in `num_classes`.  We may wish to alter the other default values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classification neural module\n",
    "cl = nemo_nlp.nm.trainables.SequenceClassifier(hidden_size=lm.hidden_size, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Loss Function\n",
    "Finally, we define the loss function neural module.  For both of our projects, we will use `CrossEntropyLossNM`.  Here's a list of what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the callable loss common neural module objects\n",
    "[method_name for method_name in dir(nemo.backends.pytorch.common.losses)\n",
    "                  if callable(getattr(nemo.backends.pytorch.common.losses, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.backends.pytorch.common.losses.CrossEntropyLossNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss function neural module\n",
    "lf = nemo.backends.pytorch.common.losses.CrossEntropyLossNM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Create Neural Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are four modules in place, defining the graph is just a matter of connecting each of the modules.  We'll define the output of each as the input of the next in the pipeline.\n",
    "\n",
    "To set up the data link, first get the outputs from the data layer. \n",
    "\n",
    "<img src=\"images/nemo/data-tensors.png\" width=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, token_types, attn_mask, labels = dl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine the embeddings, or outputs from the language model, based on the data inputs provided by the data layer.\n",
    "\n",
    "<img src=\"images/nemo/embedding-tensors.png\" width=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = lm(input_ids=input_data, token_type_ids=token_types, attention_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, define the logits (the raw, non-normalized prediction tensors) generated by the classifier.  The loss function outputs are dependent upon both the logits and the labels available from the data layer.\n",
    "\n",
    "<img src=\"images/nemo/logits-tensors.png\" width=320>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cl(hidden_states=embeddings)\n",
    "loss=lf(logits=logits, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  Now that a graph is defined and ready for action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Start an Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions of training and inference are run from the `NeuralModuleFactory` that you instantiated at the start of the exercise.  Though we don't need to actually execute the actions in this notebook, take a look at the method signatures to see what kind of information we'll need to provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.NeuralModuleFactory.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.NeuralModuleFactory.infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the setup for training in the project notebooks, you'll need to specify some functions for optimization, learning rate policy, and callbacks. The callbacks are invoked during the processing for purposes or recording intermediate parameters.  This is useful for saving checkpoints and monitoring results.  You can read more about them in the the [callbacks section of the NVIDIA NeMo User Guide](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/v0.11.0/tutorials/callbacks.html?highlight=callbacks#built-in-callbacks).  The signatures can be viewed below, and actual use examples are provided in the project notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.SimpleLogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.TensorboardLogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.EvaluatorCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.CheckpointCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've explored the NeMo API and learned:\n",
    "* The three parts of a project in NeMo are: neural modules, neural graphs, and an action\n",
    "* Neural modules we need for these projects are data layers, language models, classifiers, and loss functions\n",
    "* Neural graphs are created by defining the inputs and outputs between the neural modules\n",
    "* Actions are `train` and `infer`\n",
    "\n",
    "You are ready to build the text classification project.<br>\n",
    "\n",
    "Move on to [3.0 Build a 3-class Text Classifier](030_TextClassification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
