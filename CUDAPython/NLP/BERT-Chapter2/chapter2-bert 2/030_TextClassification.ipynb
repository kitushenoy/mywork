{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Build a 3-Class Text Classifier\n",
    "\n",
    "In this notebook, you'll build an application to classify medical disease abstracts into one of three categories: cancer diseases, neurological diseases and disorders, and \"other\" for anything else.\n",
    "\n",
    "**[3.1 Set Up the Project](#3.1-Set-Up-the-Project)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Input Parameters](#3.1.1-Input-Parameters)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Prepare the Input Data](#3.1.2-Prepare-the-Input-Data)<br>\n",
    "**[3.2 Exercise: Create Neural Modules](#3.2-Exercise:-Create-Neural-Modules)**<br>\n",
    "**[3.3 Create Neural Graphs](#3.3-Create-Neural-Graphs)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.1 Training Graph](#3.3.1-Training-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.2 Exercise: Create the Validation Graph](#3.3.2-Exercise:-Create-the-Validation-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.3 Visualize Embeddings Before Training](#3.3.3-Visualize-Embeddings-Before-Training)<br>\n",
    "**[3.4 Training](#3.4-Training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.1 Set the Learning Rate and Optimizer](#3.4.1-Set-the-Learning-Rate-and-Optimizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.2 Create the Callbacks](#3.4.2-Create-the-Callbacks)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.3 Run the Trainer](#3.4.3-Run-the-Trainer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.4 Visualize Embeddings After Training](#3.4.4-Visualize-Embeddings-After-Training)<br>\n",
    "**[3.5 Inference](#3.5-Inference)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.1 Exercise: Create the Test Graph](#3.5.1-Exercise:-Create-the-Test-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.2 Run Inference on the Test Set](#3.5.2-Run-Inference-on-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.3 Inference Results](#3.5.3-Inference-Results)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.4 Single Sentence Classification](#3.5.4-Single-Sentence-Classification)<br>\n",
    "**[3.6 Exercise: Change the Language Model](#3.6-Exercise:-Change-the-Language-Model)**<br>\n",
    "\n",
    "As presented in the [1.0 Explore the Data](010_ExploreData.ipynb) notebook, the data for the project includes training and evaluation sets already labeled, and a test set with no labels, which will need to be fixed. You'll follow the steps outlined in the [2.0 NLP Projects with NeMo](020_ExploreNeMo.ipynb) to build your application, train it, and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Set Up the Project\n",
    "\n",
    "You've learned the general paradigm for creating NLP tasks with NVIDIA NeMo.  Start by importing all the specific libraries, objects, and functions you'll need for text classification using a pre-trained language model.\n",
    "\n",
    "<img src=\"../images/nemo/nm-pipe.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful math and utility libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Import the nemo toolkit and NLP libraries\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "\n",
    "# Import the specific neural modules we need\n",
    "from nemo.collections.nlp.nm.data_layers import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import get_pretrained_lm_model, SequenceClassifier\n",
    "from nemo.backends.pytorch.common.losses import CrossEntropyLossNM\n",
    "    \n",
    "# Import helpers for fetching the learning rate and tokenizer functions\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.data.tokenizers import get_tokenizer\n",
    "\n",
    "# Import callbacks and callback functions\n",
    "from nemo.core import SimpleLogger, TensorboardLogger, EvaluatorCallback, CheckpointCallback\n",
    "from nemo.collections.nlp.callbacks.text_classification_callback import eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Input Parameters\n",
    "For reusability and convenience, set up all of the parameters in one place at the beginning of the notebook.  This way, it will be easy to make changes when you want to try alternate models and configurations. \n",
    "\n",
    "### Model Choice\n",
    "There are lots of models to choose from. Recall that the pretrained language models we are starting from were trained in a self-supervised manner over general text.  The models vary in size, and thus in memory and processing requirements. In addition, some were trained on \"uncased\" data, where all capital letters have been changed to lower case.  This results in a smaller vocabulary file, and is generally fine for many tasks, but may not be best for tasks where casing matters, such as NER.  These are some of the tradeoffs you should consider in choosing your model.  The table below gives statistics for a few of the NeMo pretrained models:\n",
    "\n",
    "| Model Name                 | Pretraining                                              | Size            |\n",
    "|----------------------------|----------------------------------------------------------|-----------------|\n",
    "| bert-base-uncased          | uncased Wikipedia and BookCorpus                         | 110M parameters |\n",
    "| bert-base-cased            | cased Wikipedia and BookCorpus                           | 110M parameters |\n",
    "| bert-large-uncased         | uncased Wikipedia and BookCorpus                         | 335M parameters |\n",
    "| bert-large-cased           | cased Wikipedia and BookCorpus                           | 335M parameters |\n",
    "| megatron-bert-345m-uncased | uncased Wikipedia, RealNews, OpenWebText, and CC-Stories | 335M parameters |\n",
    "| megatron-bert-345m-cased   | cased Wikipedia, RealNews, OpenWebText, and CC-Stories   | 335M parameters |\n",
    "\n",
    "For more discussion on pretrained models and their parameters, please see the blog post, [\"State-of-the-Art Language Modeling Using Megatron on the NVIDIA A100 GPU\"](https://developer.nvidia.com/blog/language-modeling-using-megatron-a100-gpu/).<br>\n",
    "The [AMP_OPTIMIZATION_LEVEL](https://nvidia.github.io/apex/amp.html#opt-levels) parameter can be used to set the mixed precision level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the input data location.\n",
    "DATA_DIR = '/dli/task/data/NCBI_tc-3/'\n",
    "\n",
    "# Identify the pretrained model and where information and checkpoints will be logged\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "WORK_DIR = '/dli/task/data/logs-tc-bbu/'\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'megatron-bert-345m-uncased'\n",
    "# WORK_DIR = '/dli/task/data/logs-tc-m345u/'\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'bert-large-uncased'\n",
    "# WORK_DIR = '/dli/task/data/logs-tc-blu/'\n",
    "\n",
    "# To use mixed precision, set AMP_OPTIMIZATION_LEVEL to 'O1' or 'O2',\n",
    "# to train without mixed precision, set it to 'O0'.\n",
    "AMP_OPTIMIZATION_LEVEL = 'O1'\n",
    "\n",
    "# Set the number of words in the sequences\n",
    "# Shorter sequences will be padded with 0s, longer ones truncated\n",
    "MAX_SEQ_LEN = 128 \n",
    "# set batch size - will need smaller 16 for 'bert-large-uncased' and 'megatron-bert-345m-uncased'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Prepare the Input Data\n",
    "In order to take advantage of NeMo's pre-built sentence classification data layer, the data should be formatted as \"sentence\\tlabel\" (sentence tab label).\n",
    "\n",
    "The training and evaluation datasets already have the correct \"sentence\\tlabel\" format and are ready to go for training, but the test set has no labels and needs to be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(DATA_DIR + 'test.tsv', sep='\\t')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label column filled with the '0' value, which will be ignored during testing.\n",
    "test_df['label'] = 0\n",
    "test_df = test_df[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should see the headings we need: \"sentence\" and \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated test file to disk\n",
    "test_df.to_csv(os.path.join(DATA_DIR, 'labeled_test.tsv'), sep='\\t', index=False)\n",
    "!ls -lh $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Exercise: Create Neural Modules\n",
    "You have everything you need to do most of this part yourself.  You're going to create the necessary neural modules for three pipelines:  training, validation, and test.  We always must begin by instantiating the `NeuralModuleFactory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural module factory\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level=AMP_OPTIMIZATION_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and test each need a data layer neural module instantiated with `BertTextClassificationDataLayer`.  They will reuse the same language and classifier neural modules.  Only the training and validation pipelines will need a loss function neural module, which will be reused as well.\n",
    "\n",
    "In the next cell, the tokenizer and training data layer neural module are instantiated to get you started.  The tokenizer is required for the data layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the data Layer neural module for training.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle and use_cache to True for training  \n",
    "USE_CACHE = True\n",
    "tokenizer = get_tokenizer(tokenizer_name='nemobert', pretrained_model_name=PRETRAINED_MODEL_NAME)\n",
    "dl_train = BertTextClassificationDataLayer(input_file=os.path.join(DATA_DIR, 'train.tsv'),\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             max_seq_length=MAX_SEQ_LEN,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             use_cache=USE_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next five cells, instantiate:\n",
    "1. Validation data layer neural module\n",
    "1. Test data layer neural module\n",
    "1. Language model neural module\n",
    "1. Text classification model neural module\n",
    "1. Loss neural module\n",
    "\n",
    "Look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines.  If you get stuck, look back at the [2.0 NLP Projects with NeMo](020_ExploreNeMo.ipynb) notebook for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the data Layer neural module for validation.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle to False (the default value) and use_cache to True for validation  \n",
    "dl_val = None #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Instantiate the data Layer neural module for testing (inference).\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle to False (the default value) and use_cache to False (the default value) for testing  \n",
    "dl_test = None #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Instantiate the Language Model with the get_pretrained_lm_model function\n",
    "#    Include the pretrained_model_name as the parameter\n",
    "lm = None #FIXME\n",
    "\n",
    "# Sanity check the number of weight parameters\n",
    "#    It should be around 110M for `bert-base-uncased`\n",
    "print(f'{PRETRAINED_MODEL_NAME} has {lm.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Instantiate the SequenceClassifier\n",
    "#    Include the hidden_size, num_classes (which is 3), num_layers (set to 2), \n",
    "#    and a dropout rate of 0.1\n",
    "lm_hidden_size = lm.hidden_size\n",
    "mlp = None #FIXME\n",
    "\n",
    "# Compared to the language model, the MLP should be tiny (only 1/2 million).\n",
    "print(f'MLP has {mlp.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CrossEntropyLossNM Loss Function - no parameters required\n",
    "loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job!  Your neural modules are set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Create Neural Graphs\n",
    "Define the neural graphs by linking the output of each neural module with the input of the next one in the pipeline.  If any of these fail, it may be because the neural module was not correctly instantiated in the previous exercise. These graphs define how data will flow through the neural modules. \n",
    "\n",
    "You will set up graphs for training and validation.  You'll set up the graph for testing (inference) after the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Training Graph\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/train_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Training Graph</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the outputs for each neural module to define the inputs of the next one in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "train_input, train_token_types, train_attn_mask, train_labels = dl_train()\n",
    "\n",
    "# Define train_embeddings from the language model, based on inputs from the data layer  \n",
    "train_embeddings = lm(input_ids=train_input,\n",
    "                      token_type_ids=train_token_types,\n",
    "                      attention_mask=train_attn_mask)\n",
    "\n",
    "# Define the train_logits from the clasifier, based on the embeddings from the language model\n",
    "train_logits = mlp(hidden_states=train_embeddings)\n",
    "\n",
    "# Define the train_loss based on the classifier logits and data layer labels\n",
    "train_loss = loss(logits=train_logits, labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Exercise: Create the Validation Graph\n",
    "The validation graph is very similar to the training graph and uses most of the same neural modules. Only the data layer module is different. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/val_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Validation Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "In the cell below, look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines (there are four of them).  If you get stuck, look back at the training graph you just set up for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "val_input, val_token_types, val_attn_mask, val_labels = None #FIXME\n",
    "\n",
    "# Define val_embeddings from the language model, based on inputs from the data layer  \n",
    "val_embeddings = None #FIXME\n",
    "\n",
    "# Define val_logits from the clasifier, based on the embeddings from the language model\n",
    "val_logits = None #FIXME\n",
    "\n",
    "# Define val_loss based on the classifier logits and data layer labels\n",
    "val_loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!  You've set up the pipelines for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 Visualize Embeddings Before Training\n",
    "Before running the trainer, lets get visual idea of how well the language model you've chosen distinguishes between classes without any training.  As a quick proxy of how well the language model recognizes content within each class, we'll choose just words that are obviously representative of each of the three types of abstracts, which we'll call our \"spectrum words\".  \n",
    "* cancer category (0): 'cancer', 'lymphoma', 'melanoma', 'breast cancer', 'carcinoma'\n",
    "* neurological disorders (1): 'parkinson disease', 'neuropathy', 'muscular dystrophy', 'dystonia', 'hodgkin disease'\n",
    "* other (2): 'bacterial infection', 'genetic disease', 'eczema', 'diabetes', 'deficiency'\n",
    "\n",
    "In this visualization, we want to see how related the category words are to each other within the language model (not the classifier), so we'll examine the embedding tensors that come out of the language model in our graph.  Since language model embeddings are 768 dimensional for BERT base and 1024 dimensional for BERT large, we can't really visualize them directly, so we'll first apply [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1) and reduce the embeddings to only two dimensions, which we can plot on a two-dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spectrum words and store them just like a test set\n",
    "spectrum_words = ['cancer', 'lymphoma', 'melanoma', 'breast cancer', 'carcinoma',\n",
    "                  'parkinson disease', 'neuropathy', 'muscular dystrophy', 'dystonia', 'hodgkin disease',\n",
    "                  'bacterial infection', 'genetic disease', 'eczema', 'diabetes', 'deficiency',]\n",
    "\n",
    "spectrum_file = os.path.join(DATA_DIR, 'cancer_neuro_other.tsv')\n",
    "with open(spectrum_file, 'w+') as f:\n",
    "    f.write('sentence\\tlabel')\n",
    "    for word in spectrum_words:\n",
    "        f.write('\\n' + word + '\\t0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the spectrum set and check to see if it is in the right format\n",
    "spectrum_df = pd.read_csv(spectrum_file, delimiter='\\t')\n",
    "print(spectrum_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a data layer just for this mini-dataset of words, and a graph to match.  We won't need the classifier or loss neural modules for this graph, because we are not training or even classifying on the words.  We are just going to see the results of the words run through the language model, to see if they group together naturally by category.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/spectrum_graph.png\" width=400>\n",
    "    <figcaption style=\"text-align:center;\">Spectrum Graph</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a data layer neural module for the spectrum words\n",
    "dl_spectrum = BertTextClassificationDataLayer(input_file=spectrum_file,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=MAX_SEQ_LEN,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the graph connections for the spectrum inference graph\n",
    "# There are only two neural modules in this graph: the data layer (dl_spectrum) and the language model already defined (lm)\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = dl_spectrum()\n",
    "spectrum_embeddings = lm(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the inference action (`nf.infer()`) through the spectrum graph, and sanity check the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a figure to display where the words fall along the two t-SNE feature vectors.  Are the grouped together nicely? We'll check again after training to see if anything has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:5, 0], tsne_spectrum[0:5, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[5:10, 0], tsne_spectrum[5:10, 1], 'bo')\n",
    "plt.plot(tsne_spectrum[10:, 0], tsne_spectrum[10:, 1], 'g+')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Training\n",
    "Now that the graphs are set up, the action can begin.  You'll train the model with the NeuralModuleFactory `.train()` function. We need to set how many epochs and GPUs we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "NUM_GPUS = 1\n",
    "\n",
    "train_data_size = len(dl_train)\n",
    "steps_per_epoch = math.ceil(train_data_size / (BATCH_SIZE * NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we also need to set the learning rate, optimizer, and callbacks for logging.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Set the Learning Rate and Optimizer\n",
    "For this project, we'll set the learning rate to 0.00005 and use a learning rate function, `WarmupAnnealing`.  We'll also use the popular [`adam`](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) optimization algorithm.  These values can be changed later if you wish.  We can see a list of available learning rate policies with the `get_all_lr_classes()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo.utils.lr_policies.get_all_lr_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LEARNING_RATE = 5e-5\n",
    "lr_policy_fn = get_lr_policy('WarmupAnnealing',\n",
    "                             total_steps=NUM_EPOCHS * steps_per_epoch,\n",
    "                             warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Create the Callbacks\n",
    "Callbacks are used to record and log metrics and save checkpoints for the training and evaluation. We use callbacks to print to screen and also to tensorboard.  Note that the eval_callback is where the validation set is tested against the trained network for each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback for simple logging\n",
    "train_callback = SimpleLogger(step_freq=steps_per_epoch)\n",
    "\n",
    "# Create callback for tensorboard logging\n",
    "tensorboard_callback = TensorboardLogger(nf.tb_writer,\n",
    "                                        step_freq=steps_per_epoch)\n",
    "\n",
    "# Create callback to evaluate each epoch against the validation data\n",
    "eval_callback = EvaluatorCallback(eval_tensors=[val_logits, val_labels],\n",
    "                                            user_iter_callback=lambda x, y: eval_iter_callback(x, y, dl_val),\n",
    "                                            user_epochs_done_callback=lambda x:\n",
    "                                                eval_epochs_done_callback(x, f'{nf.work_dir}graphs'),\n",
    "                                            tb_writer=nf.tb_writer,\n",
    "                                            eval_step=steps_per_epoch)\n",
    "\n",
    "# Create callback to save checkpoints\n",
    "ckpt_callback = CheckpointCallback(folder=nf.checkpoint_dir,\n",
    "                                             epoch_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Run the Trainer\n",
    "It is not necessary to reset the trainer the first time, but the reset function is included here in case you want to change the number of epochs for more training. Reset is required in order to start the trainer again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo.core.NeuralModuleFactory.reset_trainer(nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nf.train(tensors_to_optimize=[train_loss],\n",
    "         callbacks=[train_callback, tensorboard_callback, eval_callback, ckpt_callback],\n",
    "         lr_policy=lr_policy_fn,\n",
    "         optimizer=OPTIMIZER,\n",
    "         optimization_params={'num_epochs': NUM_EPOCHS, 'lr': LEARNING_RATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 Visualize Embeddings After Training\n",
    "Update the embeddings with the newly trained language model and run the inference again.  Then, show the t-SNE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings = lm(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:5, 0], tsne_spectrum[0:5, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[5:10, 0], tsne_spectrum[5:10, 1], 'bo')\n",
    "plt.plot(tsne_spectrum[10:, 0], tsne_spectrum[10:, 1], 'g+')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Inference\n",
    "Now that the language model and classifier are trained with new weights, you can set up the training graph. Then, you can run inference on the test data and look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1 Exercise: Create the Test Graph\n",
    "The test graph is a little different than the training and validation graphs.  For inference, we don't have any labels to learn from, and there is no loss function.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/test_graph.png\" width=600>\n",
    "    <figcaption style=\"text-align:center;\">Test Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "In the cell below, look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines (there are three of them).  If you get stuck, look back at the training graph you just set up for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "test_input, test_token_types, test_attn_mask, _ = None #FIXME\n",
    "\n",
    "# Define test_embeddings from the language model, based on inputs from the data layer  \n",
    "test_embeddings = None #FIXME\n",
    "\n",
    "# Define test_logits from the clasifier, based on the embeddings from the language model\n",
    "test_logits = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 Run Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_logits_tensors = nf.infer(tensors=[test_logits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the probabilities produced for each of the three labels on each test abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = [torch.nn.functional.softmax(torch.cat(test_logits_tensors[0]), dim=1).numpy()[:, label_index] for label_index in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'labeled_test.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['prob'] = test_probs \n",
    "test_df['prob0'] = test_probs[0] \n",
    "test_df['prob1'] = test_probs[1]\n",
    "test_df['prob2'] = test_probs[2]\n",
    "inference_file = os.path.join(DATA_DIR, 'test_inference.tsv')\n",
    "test_df.to_csv(inference_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll create a function to use in a loop to output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classification(sample):\n",
    "    sentence = sample.sentence\n",
    "    prob0 = sample.prob0.values[0]\n",
    "    prob1 = sample.prob1.values[0]\n",
    "    prob2 = sample.prob2.values[0]\n",
    "    result = f'{sentence} \\n cancer | neurological | other\\n {prob0} | {prob1} | {prob2}'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.3 Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(inference_file, sep='\\t')\n",
    "for ind in range(len(df)):\n",
    "    sample = df[ind:ind+1]\n",
    "    print(sample_classification(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical results should looks something like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Risk reversals in predictive testing for Huntington disease. The first predictive testing for Huntington disease ( HD ) was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD . Limits to accuracy included recombination between the DNA markers and the mutation , pedigree structure , and whether DNA samples were available from family members . With direct tests for the HD mutation , we have assessed the accuracy of results obtained by linkage approaches when requested to do so by the test individuals . For six such individuals , there was significant disparity between the tests . Three went from a decreased risk to an increased risk , while in another three the risk was decreased . Knowledge of the potential reasons for these changes in results and impact of these risk reversals on both patients and the counseling team can assist in the development of strategies for the prevention and , where necessary , management of a risk reversal in any predictive testing program . . \n",
    "Name: sentence, dtype: object \n",
    " cancer | neurological | other\n",
    " 0.020393232 | 0.88683945 | 0.092767425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.4 Single Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(nf, tokenizer, bert, mlp, sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tmp_file = \"/tmp/tmp_sentence.tsv\"\n",
    "    with open(tmp_file, 'w+') as tmp_tsv:\n",
    "        header = 'sentence\\tlabel\\n'\n",
    "        line = sentence + '\\t0\\n'\n",
    "        tmp_tsv.writelines([header, line])\n",
    "\n",
    "    tmp_data = BertTextClassificationDataLayer(input_file=tmp_file,\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               max_seq_length=128,\n",
    "                                               batch_size=1)\n",
    "    \n",
    "    tmp_input, tmp_token_types, tmp_attn_mask, _ = tmp_data()\n",
    "    tmp_embeddings = bert(input_ids=tmp_input,\n",
    "                          token_type_ids=tmp_token_types,\n",
    "                          attention_mask=tmp_attn_mask)\n",
    "    tmp_logits = mlp(hidden_states=tmp_embeddings)\n",
    "    tmp_logits_tensors = nf.infer(tensors=[tmp_logits, tmp_embeddings])\n",
    "    tmp_probs0 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 0]\n",
    "    tmp_probs1 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 1]\n",
    "    tmp_probs2 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 2]\n",
    "    print(f'\\n******\\n{sentence} \\n {tmp_probs0[0]} | {tmp_probs1[0]} | {tmp_probs2[0]}\\n********\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = ['In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia .',\n",
    "             'The first predictive testing for Huntington disease ( HD ) was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD .',\n",
    "             'Germ-line mutations of the BRCA1 gene predispose women to early-onset breast and ovarian cancer by compromising the genes presumptive function as a tumor suppressor .',\n",
    "             'Further studies suggested that low dilutions of C5D serum contain a factor ( or factors ) interfering at some step in the hemolytic assay of C5 , rather than a true C5 inhibitor or inactivator .'\n",
    "            ]\n",
    "# should be 0, 1, 0, 2 \n",
    "for sentence in sentences:\n",
    "    classify_sentence(nf, tokenizer, lm, mlp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Exercise: Change the Language Model\n",
    "Now that you've built the project, you can experiment with different settings, especially different language models.  For this exercise, try 'bert-large-uncased' or 'megatron-uncased'.  To do that, you'll need to restart the kernel to clear memory, and change the parameters in the [3.1.1 Input Parameters](3.1.1-Input-Parameters) section.  With these larger models, you'll need to reduce the batch size to avoid an out-of-memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've built a text classifier with three classes and learned:\n",
    "* How to fine-tune a BERT-based language model for text classification in NeMo\n",
    "* How to select different pre-trained language models\n",
    "* How to visualize model accuracy\n",
    "* How to test your model using inference\n",
    "\n",
    "You're ready to try a different NLP task.<br>\n",
    "\n",
    "Move on to [4.0 Build a Named Entity Recognizer](040_NamedEntityRecognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
