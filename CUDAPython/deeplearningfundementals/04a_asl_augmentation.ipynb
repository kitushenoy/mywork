{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've selected a model architecture that vastly improves the model's performance, as it is designed to recognize important features in the images. The validation accuracy is still lagging behind the training accuracy, which is a sign of overfitting: the model is getting confused by things it has not seen before when it tests against the validation dataset.\n",
    "\n",
    "In order to teach our model to be more robust when looking at new data, we're going to programmatically increase the size and variance in our dataset. This is known as [*data augmentation*](https://link.springer.com/article/10.1186/s40537-019-0197-0), a useful technique for many deep learning applications.\n",
    "\n",
    "The increase in size gives the model more images to learn from while training. The increase in variance helps the model ignore unimportant features and select only the features that are truly important in classification, allowing it to generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will be able to:\n",
    "* Augment the ASL dataset\n",
    "* Use the augmented data to train an improved model\n",
    "* Save the well-trained model to disk for use in deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're in a new notebook, we will load and process our data again. To do this, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "# Load in our data from CSV files\n",
    "train_df = pd.read_csv(\"asl_data/sign_mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"asl_data/sign_mnist_test.csv\")\n",
    "\n",
    "# Separate out our target values\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "del train_df['label']\n",
    "del test_df['label']\n",
    "\n",
    "# Separate our our image vectors\n",
    "x_train = train_df.values\n",
    "x_test = test_df.values\n",
    "\n",
    "# Turn our scalar targets into binary categories\n",
    "y_train = keras.utils.to_categorical(y_train, 25)\n",
    "y_test = keras.utils.to_categorical(y_test, 25)\n",
    "\n",
    "# Normalize our image data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape the image data for the convolutional network\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to create our model again. To do this, execute the following cell. You will notice this is the same model architecture as the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "\n",
    "num_classes = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = num_classes , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before compiling the model, it's time to set up our data augmentation.\n",
    "\n",
    "Keras comes with an image augmentation class called `ImageDataGenerator`. We recommend checking out the [documentation here](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). It accepts a series of options for augmenting your data. Later in the course, we'll have you select a proper augmentation strategy. For now, take a look at the options we've selected below, and then execute the cell to create an instance of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images horizontally\n",
    "        vertical_flip=False)  # Don't randomly flip images vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to think about why we would want to flip images horizontally, but not vertically. When you have an idea, reveal the text below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is pictures of hands signing the alphabet. If we want to use this model to classify hand images later, it's unlikely that those hands are going to be upside-down, but, they might be left-handed. This kind of domain-specific reasoning can help make good decisions for your own deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data to the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the generator must be fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data generator instance created and fit to the training data, the model can now be compiled in the same way as our earlier examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using an image data generator with Keras, a model trains a bit differently: instead of just passing the `x_train` and `y_train` datasets into the model, we pass the generator in, calling the generator's [`flow`](https://keras.io/api/preprocessing/image/) method. This causes the images to get augmented live and in memory right before they are passed into the model for training.\n",
    "\n",
    "Generators can supply an indefinite amount of data, and when we use them to train our data, we need to explicitly set how long we want each epoch to run, or else the epoch will go on indefinitely, with the generator creating an indefinite number of augmented images to provide the model.\n",
    "\n",
    "We explicitly set how long we want each epoch to run using the `steps_per_epoch` named argument. Because `steps * batch_size = number_of_images_trained in an epoch` a common practice, that we will use here, is to set the number of steps equal to the non-augmented dataset size divided by the batch_size (which has a default value of 32).\n",
    "\n",
    "Run the following cell to see the results. You will notice the training will take longer than before which makes sense given we are now training on more data than previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "858/857 [==============================] - 9s 11ms/step - loss: 1.1177 - accuracy: 0.6421 - val_loss: 1.2546 - val_accuracy: 0.6189\n",
      "Epoch 2/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.3223 - accuracy: 0.8900 - val_loss: 0.1521 - val_accuracy: 0.9439\n",
      "Epoch 3/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.1969 - accuracy: 0.9353 - val_loss: 0.2319 - val_accuracy: 0.9161\n",
      "Epoch 4/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.1498 - accuracy: 0.9520 - val_loss: 0.4362 - val_accuracy: 0.8557\n",
      "Epoch 5/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.1237 - accuracy: 0.9599 - val_loss: 0.3515 - val_accuracy: 0.8947\n",
      "Epoch 6/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.1066 - accuracy: 0.9663 - val_loss: 0.7433 - val_accuracy: 0.8310\n",
      "Epoch 7/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0984 - accuracy: 0.9691 - val_loss: 0.3471 - val_accuracy: 0.8991\n",
      "Epoch 8/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0856 - accuracy: 0.9745 - val_loss: 0.0769 - val_accuracy: 0.9718\n",
      "Epoch 9/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0786 - accuracy: 0.9750 - val_loss: 0.6483 - val_accuracy: 0.8504\n",
      "Epoch 10/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 1.6467 - val_accuracy: 0.7553\n",
      "Epoch 11/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0730 - accuracy: 0.9788 - val_loss: 0.1290 - val_accuracy: 0.9632\n",
      "Epoch 12/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0695 - accuracy: 0.9801 - val_loss: 0.1298 - val_accuracy: 0.9735\n",
      "Epoch 13/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0638 - accuracy: 0.9819 - val_loss: 0.1712 - val_accuracy: 0.9470\n",
      "Epoch 14/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0578 - accuracy: 0.9834 - val_loss: 0.0843 - val_accuracy: 0.9763\n",
      "Epoch 15/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0628 - accuracy: 0.9830 - val_loss: 0.1846 - val_accuracy: 0.9491\n",
      "Epoch 16/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0588 - accuracy: 0.9844 - val_loss: 1.3783 - val_accuracy: 0.7687\n",
      "Epoch 17/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0561 - accuracy: 0.9850 - val_loss: 0.4901 - val_accuracy: 0.8833\n",
      "Epoch 18/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0578 - accuracy: 0.9842 - val_loss: 2.4703 - val_accuracy: 0.7126\n",
      "Epoch 19/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0557 - accuracy: 0.9839 - val_loss: 0.0158 - val_accuracy: 0.9934\n",
      "Epoch 20/20\n",
      "858/857 [==============================] - 8s 10ms/step - loss: 0.0498 - accuracy: 0.9861 - val_loss: 0.0757 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb00416f7b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(x_train,y_train, batch_size=32), # Default batch_size is 32. We set it here for clarity.\n",
    "          epochs=20,\n",
    "          steps_per_epoch=len(x_train)/32, # Run same number of steps we would if we were not using a generator.\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the validation accuracy is higher, and more consistent. This means that our model is no longer overfitting in the way it was; it generalizes better, making better predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a well-trained model, we will want to deploy it to perform inference on new images.\n",
    "\n",
    "It is common, once we have a trained model that we are happy with to save it to disk.\n",
    "\n",
    "Saving the model in Keras is quite easy using the save method. There are different formats that we can save in, but we'll use the default for now. If you'd like, feel free to check out [the documentation](https://www.tensorflow.org/guide/keras/save_and_serialize). In the next notebook, we'll load the model and use it to read new sign language pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: asl_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('asl_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you used Keras to augment your dataset, the result being a trained model with less overfitting and excellent test image results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear the Memory\n",
    "Before moving on, please execute the following cell to clear up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a well-trained model saved to disk, you will, in the next section, deploy it to make predictions on not-yet-seen images.\n",
    "\n",
    "Please continue to the next notebook: [*Model Predictions*](04b_asl_predictions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
